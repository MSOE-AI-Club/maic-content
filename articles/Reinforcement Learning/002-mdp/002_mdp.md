<br>
<a href='/learning-tree?node=66' style='
    background-color: #31313a;
    color: gainsboro;
    padding: 6px 16px;
    border: none
    border-radius: 4px;
    text-transform: uppercase;
    font-family: "Roboto", sans-serif;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    text-decoration: none;
    display: inline-block;'
>
  View in Learning Tree
</a>

<br>
<br>
<br>

<div style='
  position: relative;
  padding: 10px; 
  border-radius: 5px;
  background-color: rgba(0, 0, 0, 0.85); 
  border: 4px solid transparent;
  background-image: linear-gradient(90deg, rgba(0, 0, 0, 0.85), rgba(0, 0, 0, 0.85)), linear-gradient(90deg, gold, orange, gold);
  background-origin: border-box;
  background-clip: padding-box, border-box;
'>

<svg width='200' height='50' style='display: block; margin-bottom: 5px;'>
  <text x='0' y='35' font-size='35' font-family='Arial' font-weight='bold' fill='gold'>
    Why Read?
    <animate attributeName='fill' values='gold; orange; gold' dur='3s' repeatCount='indefinite' />
  </text>
</svg>

<p style='color: white; margin-top: 2px;'>
Markov Decision Processes (MDPs) are the mathematical backbone of reinforcement learning. They give us a formal way to describe environments, actions, and rewards, or the “rules of the game” an agent learns in. If you want to really understand RL (beyond just running a Gym environment and hoping it trains), MDPs are where it starts.
</p>
</div>
<br/>
<br/>


## Introduction to Markov Decision Processes (MDPs)

A Markov Decision Process (MDP) is a framework for modeling situations where an agent interacts with an environment over time and receives rewards based on its actions. The key assumption is the **Markov property**: the idea that the next state depends only on the current state and action, not on the full history.

***In RL terms:***  
> The agent doesn’t need to remember everything. It just needs the right state representation.

MDPs help us formalize RL problems so we can reason about policies, value functions, and optimal behavior.


## Components of an MDP
An MDP is defined by the following components:

- **States (S)** – all possible situations the agent can be in  
- **Actions (A)** – the choices available to the agent  
- **Transition Function (P(s' | s, a))** – probability of ending up in state `s'` after taking action `a` in state `s`  
- **Reward Function (R(s, a))** – the immediate feedback for a state–action pair  

Together, these define the environment the agent operates in.


## The Goal: Find the Best Policy

The purpose of the MDP framework is to formalize the problem of finding a **policy** for the agent.

The agent's objective is to find a sequence of actions that maximizes the accumulated **reward** over the long run. This accumulated reward is called the **return**. Since the future is uncertain, we use a **discount factor ($\gamma$)** to make immediate rewards more valuable than distant future rewards.

Solving an MDP means finding the **optimal policy** ($\pi^*$). To achieve this, Reinforcement Learning algorithms rely on two fundamental concepts: the **Policy** and the **Value Function**.

### Policy ($\pi$)

The **Policy** is the agent's brain. It is a function that maps the agent's current **state (s)** to the **action (a)** it should take.


### Value Functions (V and Q)

Value functions are used to evaluate the goodness of states and actions under a given policy.


- **State-Value ($V^{\pi}(s)$):** how good it is to be in state `s` under policy `π`  
- **Action-Value ($Q^{\pi}(s, a)$):** how good it is to take action `a` in state `s` and then follow `π`

Many classical RL algorithms (Value Iteration, Policy Iteration) use the **Bellman equations** to compute these values and converge to the optimal policy $\pi^*$.




## Example: Grid World

Imagine a simple grid where an agent can move North, South, East, or West.

- **States (S):** each grid cell (e.g., (1, 1))  
- **Actions (A):** move directions  
- **Transition Function (P):** “move north” usually goes north, but randomness might send you sideways  
  - e.g., 0.8 → north, 0.2 → east  
- **Reward Function (R):**
  - +10 for goal  
  - –10 for trap  
  - –1 for each move (to encourage efficiency)

The **optimal policy** is the set of arrows on every cell that guides the agent to the +10 goal cell while avoiding the -10 trap cell, thus maximizing its total score.


# Resources on Markov Decision Processes (MDPs)

## What Is the Markov Decision Process? (Spiceworks)
<!-- resource_url: https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/ -->
**Best for:** Practical explanations + real-world examples  
Clear intro that walks through MDP components and shows real-life applications. Good next step after understanding definitions.

<a href='https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/' style='
    margin-top: 4px;
    background-color: #31313a;
    color: gainsboro;
    padding: 6px 16px;
    text-transform: uppercase;
    font-family: "Roboto", sans-serif;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    display: inline-block;'
/> What Is the Markov Decision Process?</a>
<br>

## Markov Decision Processes in 5 Minutes (Cyrill Stachniss)
<!-- resource_url: https://www.youtube.com/watch?v=4Fqt2Nk2lhY -->
**Best for:** Fast, visual understanding  
Short, clean explainer video. This is great if you're a visual learner or want a quick refresher before diving into code.

<a href='https://www.youtube.com/watch?v=4Fqt2Nk2lhY' style='
    margin-top: 4px;
    background-color: #31313a;
    color: gainsboro;
    padding: 6px 16px;
    text-transform: uppercase;
    font-family: "Roboto", sans-serif;
    font-size: 1em;
    font-weight: bold;
    cursor: pointer;
    display: inline-block;'
/> MDPs in 5 Minutes</a>





